{"cells":[{"cell_type":"markdown","metadata":{"id":"SKY7RA4R1n7R"},"source":["# Humour Classification"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27157,"status":"ok","timestamp":1701837888597,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"Rsfu8iuvNBsE","outputId":"f7fc54ae-45c1-4951-9b53-05b239cba347"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["!pip install gensim\n","import re\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from collections import defaultdict\n","\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","\n","import gensim\n","from gensim import corpora\n","from gensim import utils\n","import numpy as np\n","import pickle\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_n4JB6HiixA1","executionInfo":{"status":"ok","timestamp":1701837903243,"user_tz":300,"elapsed":205,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"S-L9DavQ2JXX"},"source":["## 1. Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":16844,"status":"ok","timestamp":1701645435747,"user":{"displayName":"Akhila Ravi","userId":"10956993517355176145"},"user_tz":300},"id":"TGve4iQ13qEH","outputId":"8fc922fc-c98c-4fad-ace0-74c4fe1b85b6"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-73bcc63d-b1e2-44ff-b218-bf95ea43c240\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-73bcc63d-b1e2-44ff-b218-bf95ea43c240\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving jokes_and_sentences.csv to jokes_and_sentences.csv\n"]}],"source":["from google.colab import files\n","uploads = files.upload()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2OKBarqiK6bF","executionInfo":{"status":"ok","timestamp":1701837932563,"user_tz":300,"elapsed":176,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["def load_jokes(filename):\n","  '''Function to load,shuffle and clean data. Incorporates clean_data() function.\n","\n","  Returns:\n","  clean(dict): shuffled and formatted data; {'Text': ['example','text'],'Target': [0,1]}\n","  '''\n","\n","  df = pd.read_csv('jokes_and_sentences.csv')\n","  #df = pd.read_csv(filename)\n","\n","  #shuffles rows of df\n","  df = df.sample(frac = 1)\n","\n","  print(df.head(20))\n","\n","  all_lines = df.to_dict()\n","  text = list(all_lines['Text'].values())\n","  labels = list(all_lines['Target'].values())\n","\n","  clean = defaultdict(list)\n","  clean['Text'] = text\n","  clean['Target'] = labels\n","\n","  for i in range(len(clean['Text'])):\n","    words = clean['Text'][i]\n","    cleaned = clean_data(words)\n","    clean['Text'][i] = cleaned\n","\n","  return clean\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Srjy_7ZTM0QA","executionInfo":{"status":"ok","timestamp":1701837934750,"user_tz":300,"elapsed":129,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["def clean_data(data):\n","  '''Function to clean text\n","  Performs lemmatization, removes non alphanumeric characters and removes stop words\n","\n","  Args:\n","  data(string): piece of text to be cleaned\n","\n","  Returns:\n","  clean_text(string): cleaned text\n","  '''\n","  # convert to lower case\n","  clean_text = data.lower()\n","  #remove hyphens and sub with space\n","  clean_text = clean_text.replace(\"-\",\" \")\n","  clean_text = clean_text.replace(\":\",\" \")\n","  clean_text = clean_text.replace(\"...\",\" \")\n","\n","  # tokenize the data\n","  clean_text = clean_text.split(\" \")\n","\n","  # remove non alphanumeric chars\n","  clean_text = \" \".join([re.sub(r'[^a-zA-Z0-9]', '', word) for word in clean_text])\n","\n","  # remove stopwords\n","  stopword = nltk.corpus.stopwords.words('english')\n","  review_cleaned = \" \".join([word for word in re.split('\\W+', clean_text) if word not in stopword])\n","\n","  # perform lemmatizing\n","  wn = nltk.WordNetLemmatizer()\n","  review_cleaned = \" \".join([wn.lemmatize(word,'v') for word in re.split('\\W+', review_cleaned)])\n","\n","  return review_cleaned"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5808,"status":"ok","timestamp":1701837944803,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"WbPwX4lO4LV5","outputId":"010c680d-ad29-4a28-a053-bb741e044284"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   Text  Target\n","7040  It is a distance race that demands a great dea...       0\n","5308  Some animals, like people and most vertebrates...       0\n","657   Wife told me to take the spider out instead of...       1\n","6563  Both come about because this part of the air g...       0\n","7064  The open world environment allows players to c...       0\n","2699  A guy stood over his tee shot for what seemed ...       1\n","81          Why do cows not have toes? They lactose! \\t       1\n","598   I tried taking some high resolution photos of ...       1\n","4871  A vaccine often contains something like a germ...       0\n","6289  And not all long sentences are run-on sentence...       0\n","1082  A man went to a restaurant and ordered a steak...       1\n","7852  Percentage is far below consensus. Please try ...       0\n","2882  It was graduation day and Mom was trying to ta...       1\n","3839  A feisty 70 year old woman had to call a furna...       1\n","3707  HOBBIES, TECHNICAL:\\n\\n\\nHOBBIES, NONTECHNICAL...       1\n","4488  En has a similar one, but it only involves edi...       0\n","434   Have you heard of the band 1023MB? They haven'...       1\n","7822  A 'Milk substitute' is a product that resemble...       0\n","227   You can't run through a camp site. You can onl...       1\n","627   \"What time is it?\" I don't know... it keeps ch...       1\n"]}],"source":["data = load_jokes('jokes_and_sentences.csv')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1701837956000,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"fV1shJx-4p57","outputId":"f54cb61a-3288-4f54-d845-e8a4de398e89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['distance race demand great deal endurance well speed strategies also play key role combination make attractive many',\n"," 'animals like people vertebrates two ears animals hear ears spiders small hairs legs hear',\n"," 'wife tell take spider instead kill drink cool guy want web developer ',\n"," 'come part air get colder go cold air become thicker fall warm air become thinner go turn earth move air well air move north south middle earth generally get power sun warmer north south point',\n"," 'open world environment allow players choose want play storyline missions progress players also take part many events come across explore world',\n"," 'guy stand tee shoot seem eternity look look measure distance figure wind direction speed drive partner nutsfinally exasperate partner say hell take long hit goddamn ballthe guy answer wife watch clubhouse want make perfect shotwell hell man dont stand snowball chance hell hit',\n"," 'cow toe lactose ',\n"," 'try take high resolution photos local farmland turn bite grainy ',\n"," 'vaccine often contain something like germ weaken kill germs toxins vaccines synthetic vaccines make laboratory',\n"," 'long sentence run sentence english teachers say run sentence wrong']"]},"metadata":{},"execution_count":7}],"source":["data['Text'][:10]"]},{"cell_type":"markdown","metadata":{"id":"dUU1UOr-2Nqb"},"source":["## 2. Text Preprocessing (Incorporated into the Load jokes function for ease of use)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"rXgUdSzb2n9O","executionInfo":{"status":"ok","timestamp":1701837990767,"user_tz":300,"elapsed":3474,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["for i in range(len(data['Text'])):\n","    words = data['Text'][i]\n","    cleaned = clean_data(words)\n","    data['Text'][i] = cleaned"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1701837992234,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"oq9T24M3CJ5s","outputId":"56da5a39-7952-4bb7-fdef-7fdf096ef3cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['distance race demand great deal endurance well speed strategies also play key role combination make attractive many',\n"," 'animals like people vertebrates two ears animals hear ears spiders small hairs legs hear',\n"," 'wife tell take spider instead kill drink cool guy want web developer ',\n"," 'come part air get colder go cold air become thicker fall warm air become thinner go turn earth move air well air move north south middle earth generally get power sun warmer north south point',\n"," 'open world environment allow players choose want play storyline missions progress players also take part many events come across explore world',\n"," 'guy stand tee shoot seem eternity look look measure distance figure wind direction speed drive partner nutsfinally exasperate partner say hell take long hit goddamn ballthe guy answer wife watch clubhouse want make perfect shotwell hell man dont stand snowball chance hell hit',\n"," 'cow toe lactose ',\n"," 'try take high resolution photos local farmland turn bite grainy ',\n"," 'vaccine often contain something like germ weaken kill germs toxins vaccines synthetic vaccines make laboratory',\n"," 'long sentence run sentence english teachers say run sentence wrong']"]},"metadata":{},"execution_count":9}],"source":["data['Text'][:10]"]},{"cell_type":"markdown","metadata":{"id":"HOgcsktfcePq"},"source":["#### 2.1 Write cleaned data to a file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kxz3c7NBcjU7"},"outputs":[],"source":["with open('cleaned_data.pkl', 'wb') as f:\n","  pickle.dump(data['Text'], f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AA4aB2qvyPJ2"},"outputs":[],"source":["with open('data_labels.pkl', 'wb') as f:\n","  pickle.dump(data['Target'], f)"]},{"cell_type":"markdown","metadata":{"id":"stgH5aWoCPjk"},"source":["## 3. Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"Sw1t14RbEcza"},"source":["Need to create document vector from the word embeddings. We should list following link in references for report:\n","https://www.kaggle.com/code/kstathou/word-embeddings-logistic-regression"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"kZ7PLS6ZEb6v","executionInfo":{"status":"ok","timestamp":1701838001472,"user_tz":300,"elapsed":148,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["def create_doc_vec(embeddings, doc, emb_size, agg=True):\n","  '''Function to take dict of word embeddings and creates document vector as an output\n","  Args:\n","    embeddings(dict): embeddings dictionary where keys correspond to words in the vocab\n","    doc(string): a single document\n","    emb_size(int): the length of an embedding vector\n","  Returns:\n","    doc_vec(array): a 2d array containing the word embeddings\n","  '''\n","  doc_ = doc.split()\n","  doc_vect = []\n","  for word in doc_:\n","    try:\n","      embed = embeddings[word]\n","      doc_vect.append(embed)\n","    except:\n","      pass\n","      #embed = [0 for i in range(emb_size)]\n","\n","  doc_vect = np.array(doc_vect)\n","  if agg:\n","    doc_vect = np.mean(doc_vect, axis=0)\n","\n","  return doc_vect\n"]},{"cell_type":"markdown","metadata":{"id":"HYep9heVCUEd"},"source":["### 3.1 TF-IDF vectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQ5vOQjQCMWc"},"outputs":[],"source":["# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Fit the vectorizer on the data\n","X_tfidf = vectorizer.fit_transform(data['Text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97,"status":"ok","timestamp":1701532592203,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"lYoQTMWWD49U","outputId":"3e438196-db47-4953-c40c-d4da5a5fd767"},"outputs":[{"data":{"text/plain":["(7880, 29369)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["X_tfidf.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0Et-aGgd-9R"},"outputs":[],"source":["# Saving the TF_IDF to a file\n","with open('embeddings_tfidf.pkl', 'wb') as f:\n","  pickle.dump(X_tfidf, f)"]},{"cell_type":"markdown","metadata":{"id":"e-p8olGSwjHy"},"source":["### 3.2 Training Word2Vec model"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":755,"status":"ok","timestamp":1701838024882,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"jgkmN0e6rLTc","outputId":"801be19b-6257-4115-c995-f5c8a78410a3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<gensim.corpora.dictionary.Dictionary at 0x78b6dd48f040>"]},"metadata":{},"execution_count":11}],"source":["dictionary = corpora.Dictionary(line.split() for line in data['Text'])\n","dictionary"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"MEi_I5AJmvKP","executionInfo":{"status":"ok","timestamp":1701838027813,"user_tz":300,"elapsed":144,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["class MyCorpus:\n","    def __iter__(self):\n","        for line in data['Text']:\n","          # assume there's one document per line, tokens separated by whitespace\n","          yield utils.simple_preprocess(line)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1701838028902,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"wPHOSnKmq3G4","outputId":"4fddeb1f-ad03-4b63-f972-d36cc01cd9a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.MyCorpus object at 0x78b6dd48fd00>\n"]}],"source":["corpus = MyCorpus()\n","print(corpus)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Fy6yLzQdrE_l","executionInfo":{"status":"ok","timestamp":1701838031515,"user_tz":300,"elapsed":394,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["for vector in corpus:\n","    continue  # load one vector into memory at a time\n","    print(vector)\n","    break\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"zkV9eBmfsqh0","executionInfo":{"status":"ok","timestamp":1701838038692,"user_tz":300,"elapsed":6247,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["model = gensim.models.Word2Vec(sentences=corpus)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1701838040200,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"cyYhs8jnYk1x","outputId":"13ae37dc-60ec-4820-adf9-08d4f6aa150a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["29382"]},"metadata":{},"execution_count":16}],"source":["vocab = set()\n","for string in data['Text']:\n","  words = string.split()\n","  vocab.update(words)\n","\n","len(vocab)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"xoX2LdACv2ti","executionInfo":{"status":"ok","timestamp":1701838041460,"user_tz":300,"elapsed":140,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"}}},"outputs":[],"source":["embeddings_trained_model_list = []\n","embeddings_trained_model = {}\n","for word in vocab:\n","  try:\n","    vec_c = model.wv[word]\n","    embeddings_trained_model_list.append(vec_c)\n","    embeddings_trained_model[word] = vec_c\n","  except KeyError:\n","    continue"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1701838043232,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"cgaxA1_vbxzi","outputId":"111793fe-6a32-48af-d085-aa69373bea86"},"outputs":[{"output_type":"stream","name":"stdout","text":["5707\n"]},{"output_type":"execute_result","data":{"text/plain":["array([100])"]},"metadata":{},"execution_count":18}],"source":["#need to check length of each vector\n","emb_len  = len(embeddings_trained_model)\n","print(emb_len)\n","le = []\n","for i in range(emb_len):\n","  le.append(len(embeddings_trained_model_list[i]))\n","le = np.array(le)\n","emb_vec_size = np.unique(le)\n","\n","emb_vec_size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":159,"status":"ok","timestamp":1701370073911,"user":{"displayName":"Akhila Ravi","userId":"10956993517355176145"},"user_tz":300},"id":"OFPsLnywtAO2","outputId":"9b12f2a7-25c3-4b4e-edd0-feec9be88686"},"outputs":[{"data":{"text/plain":["array([-0.14696124,  0.36991766,  0.09362533, -0.1108419 ,  0.08769319,\n","       -0.5812112 ,  0.16687095,  0.7445814 , -0.22547688, -0.2111091 ,\n","       -0.13856097, -0.5161872 , -0.04633697, -0.01273117,  0.02413861,\n","       -0.33954874, -0.09285438, -0.38825518,  0.07410315, -0.6231759 ,\n","        0.19963343,  0.221795  ,  0.4367928 , -0.14297172, -0.1284446 ,\n","        0.11873754, -0.23265192, -0.19718258, -0.16379066,  0.06883798,\n","        0.4861388 ,  0.12306323,  0.09142508, -0.15280724,  0.14072526,\n","        0.3024679 , -0.08148299, -0.35445035, -0.2318539 , -0.60254806,\n","        0.10525856, -0.22507106, -0.28386873,  0.07236446,  0.17296818,\n","       -0.36758518, -0.3272282 , -0.11837509,  0.16549821,  0.23980342,\n","        0.09453598, -0.4486291 , -0.08290415, -0.104697  , -0.35850808,\n","        0.30226302,  0.2999139 , -0.1016961 , -0.38924235,  0.20495047,\n","        0.11201318, -0.05786983, -0.13116279, -0.01444412, -0.47610587,\n","        0.21180329,  0.12212019,  0.2593602 , -0.4683321 ,  0.46431458,\n","       -0.18920279,  0.21851015,  0.32206598, -0.24435058,  0.31198278,\n","        0.13711067, -0.05391038, -0.06437777, -0.19078542,  0.1533242 ,\n","       -0.3551573 , -0.05702978, -0.24024034,  0.3658649 , -0.09391029,\n","        0.06627595, -0.06530228,  0.3643629 ,  0.47301897,  0.03644183,\n","        0.54265183,  0.04581374,  0.01230159,  0.09016179,  0.55450433,\n","        0.18795025,  0.13189547, -0.42559072,  0.1343377 , -0.06544557],\n","      dtype=float32)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["vec_test = model.wv['king']\n","vec_test"]},{"cell_type":"markdown","metadata":{"id":"BLa7TN7TfTxa"},"source":["#### 3.2.1 Creating document vector for Logistic Regression"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":463,"status":"ok","timestamp":1701838049015,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"67KJaEeJIFFu"},"outputs":[],"source":["#create document vector for logistic regression\n","total_vect = []\n","for doc in data['Text']:\n","  vec = create_doc_vec(embeddings_trained_model, doc, int(emb_vec_size))\n","  total_vect.append(vec)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1701838052555,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"lxciNtnqeWrl","outputId":"446dc89b-307c-45b0-fa8b-22b39dcb8673"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7880, 100)"]},"metadata":{},"execution_count":20}],"source":["#check shape of document vector\n","total_vect = np.array(total_vect)\n","total_vect.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wxUfw9Ceqsf"},"outputs":[],"source":["# Save document vector for logistic regression\n","with open('embeddings_trained_lr.pkl', 'wb') as f:\n","  pickle.dump(total_vect, f)"]},{"cell_type":"markdown","metadata":{"id":"g37SV4cDfaw3"},"source":["#### 3.2.2 Creating Document Vector for LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWalcqylvFNF"},"outputs":[],"source":["# Creating document vectors for model trained on our data for LSTM\n","embeddings_implemented_doc_vec = []\n","\n","for doc in data['Text']:\n","  vec = create_doc_vec(embeddings_trained_model, doc, int(emb_vec_size), False)\n","  embeddings_implemented_doc_vec.append(vec)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyMEBMQxviUH"},"outputs":[],"source":["# Save the document embeddings obtained from model trained on our data\n","with open('embeddings_trained_lstm.pkl', 'wb') as f:\n","  pickle.dump(embeddings_implemented_doc_vec, f)\n"]},{"cell_type":"markdown","metadata":{"id":"9OubQiwqftVJ"},"source":["--------------------"]},{"cell_type":"markdown","metadata":{"id":"Jg0-Ic2IEiIt"},"source":["### 3.3 Pre-trained Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"d2Ybk49rJozc"},"source":["We will fetch the Word2Vec model trained on part of the Google News dataset covering approximately 3 million words and phrases. Such a model can take hours to train, but since it’s already available, downloading and loading it will be much faster."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3DgT378WD-w8","outputId":"90e95371-4ef9-4a77-ea1c-ecf657fdf8ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}],"source":["# Pretrained Word2Vec model\n","from gensim.models import Word2Vec\n","import gensim.downloader as api\n","\n","wv = api.load('word2vec-google-news-300')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WQ7wLi3vXy73","outputId":"c97f108a-47f2-4a08-d7ec-3e9502f43adf"},"outputs":[{"data":{"text/plain":["29382"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["vocab = set()\n","for string in data['Text']:\n","  words = string.split()\n","  vocab.update(words)\n","\n","len(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i33irEBzgA_n"},"outputs":[],"source":["embeddings_pretrained_model = {}\n","embeddings_pretrained_model_list = []\n","for word in vocab:\n","  try:\n","    vec_c = wv[word]\n","    embeddings_pretrained_model_list.append(vec_c)\n","    embeddings_pretrained_model[word] = vec_c\n","  except KeyError:\n","    continue"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IFBA3EQCiFzJ","outputId":"4cdf38df-99f6-4e75-8425-02dfacce0a7b"},"outputs":[{"data":{"text/plain":["16613"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["len(embeddings_pretrained_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"64HrhU-zXWKw","outputId":"894331d3-52ba-4bf8-b8c6-a5f0a7996937"},"outputs":[{"name":"stdout","output_type":"stream","text":["16613\n","[300]\n"]}],"source":["#need to check length of each vector\n","emb_len = len(embeddings_pretrained_model)\n","print(emb_len)\n","le = []\n","\n","for i in range(emb_len):\n","  le.append(len(embeddings_pretrained_model_list[i]))\n","\n","le = np.array(le)\n","emb_vec_size = np.unique(le)\n","print(emb_vec_size)"]},{"cell_type":"markdown","metadata":{"id":"ocsMfmCxgKXs"},"source":["#### 3.3.1 Creating Document Vector for Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzkArzqDXOsL"},"outputs":[],"source":["#create document vector for logistic regression\n","total_pre_vect = []\n","for doc in data['Text']:\n","  vec = create_doc_vec(embeddings_pretrained_model, doc, int(emb_vec_size))\n","  total_pre_vect.append(vec)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1701533403599,"user":{"displayName":"Surabhi Kulkarni","userId":"06760744003477857285"},"user_tz":300},"id":"I9HlUFt3XVXk","outputId":"d8eb6aaf-0687-4276-9898-4b5388aa1df8"},"outputs":[{"data":{"text/plain":["(7880, 300)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["#check shape of document vector\n","total_pre_vect = np.array(total_pre_vect)\n","total_pre_vect.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKx0RpuNgV8F"},"outputs":[],"source":["# Save document vector for LR in a file\n","with open('embeddings_pretrained_lr.pkl', 'wb') as f:\n","  pickle.dump(total_pre_vect, f)\n"]},{"cell_type":"markdown","metadata":{"id":"EWfauuH4olN7"},"source":["##### Create Document vectors for LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Nk4u6Ifvoo-O"},"outputs":[],"source":["# Creating document vectors for model pretrained for LSTM\n","embeddings_pretrained_doc_vec = []\n","\n","for doc in data['Text']:\n","  vec = create_doc_vec(embeddings_pretrained_model, doc, int(emb_vec_size), False)\n","  embeddings_pretrained_doc_vec.append(vec)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Rl37JETgp-hg","outputId":"4b5d56bc-d5cf-4c93-d053-2e2b9dafe54f"},"outputs":[{"data":{"text/plain":["7880"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["len(embeddings_pretrained_doc_vec)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KdqzgPNmuz73"},"outputs":[],"source":["# Save the document embeddings obtained from pretrained model\n","with open('embeddings_pretrained_lstm.pkl', 'wb') as f:\n","  pickle.dump(embeddings_pretrained_doc_vec, f)\n"]},{"cell_type":"markdown","metadata":{"id":"BUUE_5Lzd5rJ"},"source":["Note: We averaged the embeddings of the words to construct a document vector of a consisent shape to pass through logistic regression. However, when doing this unfortunately the relationships between each word can get lost. However, we end up with a new/different representation of the words than TFIDF to pass through logistic regression\n","\n","Due to these limitations, Doc2Vec might be more suitable for logistic regression since it does paragraph embeddings\n","https://stats.stackexchange.com/questions/299446/word-embeddings-with-logistic-regression"]},{"cell_type":"markdown","metadata":{"id":"-5BmvP_qc7v7"},"source":["Using the pre-trained model, we can get word embeddings for 16k words as other words are unknown to the pretrained model.\n","\n","Pre-trained model gives us embeddings for more words compared to the model we trained.\n","\n","-----------------------------"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
